{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf5CmXQCZyF1"
   },
   "source": [
    "# Guided Capstone Step 4. Pre-Processing and Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2jue2jPGJlt"
   },
   "source": [
    "**The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    " \n",
    "3.   Exploratory Data Analysis   \n",
    "\n",
    "4.   **Pre-processing and Training Data Development**  \n",
    " * Create dummy or indicator features for categorical variables\n",
    "  * Standardize the magnitude of numeric features\n",
    "  * Split into testing and training datasets\n",
    "  * Apply scaler to the testing set\n",
    "5.   Modeling \n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes — Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8xfkAqqZyF2"
   },
   "source": [
    "**<font color='teal'> Start by loading the necessary packages as we did in step 3 and printing out our current working directory just to confirm we are in the correct project directory. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry6WPL5eZyF3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['updated_ski_data.csv',\n",
       " '.DS_Store',\n",
       " 'GuidedCapstoneStep4 and step5HL.ipynb',\n",
       " 'step2_output.csv',\n",
       " 'GuidedCapstoneStep3HL.ipynb',\n",
       " 'GuidedCapstone_Step2HL.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'step3_output.csv',\n",
       " '1564524869_Problem_Statement_Worksheet_Template.pptx']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "630T-ogRZyF8"
   },
   "source": [
    "**<font color='teal'>  Load the csv file you created in step 3, remember it should be saved inside your data subfolder and print the first five rows.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMNbk0u3ZyF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 176 entries, 0 to 175\n",
      "Data columns (total 27 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Name               176 non-null    object \n",
      " 1   state              176 non-null    object \n",
      " 2   summit_elev        176 non-null    int64  \n",
      " 3   vertical_drop      176 non-null    int64  \n",
      " 4   base_elev          176 non-null    int64  \n",
      " 5   trams              176 non-null    float64\n",
      " 6   fastEight          176 non-null    float64\n",
      " 7   fastSixes          176 non-null    float64\n",
      " 8   fastQuads          176 non-null    int64  \n",
      " 9   quad               176 non-null    int64  \n",
      " 10  triple             176 non-null    int64  \n",
      " 11  double             176 non-null    int64  \n",
      " 12  surface            176 non-null    int64  \n",
      " 13  total_chairs       176 non-null    int64  \n",
      " 14  Runs               176 non-null    float64\n",
      " 15  TerrainParks       176 non-null    float64\n",
      " 16  LongestRun_mi      176 non-null    float64\n",
      " 17  SkiableTerrain_ac  176 non-null    float64\n",
      " 18  Snow Making_ac     176 non-null    float64\n",
      " 19  daysOpenLastYear   176 non-null    float64\n",
      " 20  yearsOpen          176 non-null    float64\n",
      " 21  averageSnowfall    176 non-null    float64\n",
      " 22  AdultWeekday       176 non-null    float64\n",
      " 23  AdultWeekend       176 non-null    float64\n",
      " 24  projectedDaysOpen  176 non-null    float64\n",
      " 25  NightSkiing_ac     176 non-null    float64\n",
      " 26  clusters           176 non-null    int64  \n",
      "dtypes: float64(15), int64(10), object(2)\n",
      "memory usage: 37.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"step3_output.csv\")\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "df.head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkBHf9smZyGB"
   },
   "source": [
    "## Create dummy features for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWKHm0NhAnrJ"
   },
   "source": [
    "**<font color='teal'> Create dummy variables for `state`. Add the dummies back to the dataframe and remove the original column for `state`. </font>**\n",
    "\n",
    "Hint: you can see an example of how to execute this in Aiden's article on preprocessing [here](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-pre-processing-and-training-data-development-fd2d75182967). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZqWk8ltZyGZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>summit_elev</th>\n",
       "      <th>vertical_drop</th>\n",
       "      <th>base_elev</th>\n",
       "      <th>trams</th>\n",
       "      <th>fastEight</th>\n",
       "      <th>fastSixes</th>\n",
       "      <th>fastQuads</th>\n",
       "      <th>quad</th>\n",
       "      <th>triple</th>\n",
       "      <th>...</th>\n",
       "      <th>Rhode Island</th>\n",
       "      <th>South Dakota</th>\n",
       "      <th>Tennessee</th>\n",
       "      <th>Utah</th>\n",
       "      <th>Vermont</th>\n",
       "      <th>Virginia</th>\n",
       "      <th>Washington</th>\n",
       "      <th>West Virginia</th>\n",
       "      <th>Wisconsin</th>\n",
       "      <th>Wyoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hilltop Ski Area</td>\n",
       "      <td>2090</td>\n",
       "      <td>294</td>\n",
       "      <td>1796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunrise Park Resort</td>\n",
       "      <td>11100</td>\n",
       "      <td>1800</td>\n",
       "      <td>9200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yosemite Ski &amp; Snowboard Area</td>\n",
       "      <td>7800</td>\n",
       "      <td>600</td>\n",
       "      <td>7200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name  summit_elev  vertical_drop  base_elev  \\\n",
       "0               Hilltop Ski Area         2090            294       1796   \n",
       "1            Sunrise Park Resort        11100           1800       9200   \n",
       "2  Yosemite Ski & Snowboard Area         7800            600       7200   \n",
       "\n",
       "   trams  fastEight  fastSixes  fastQuads  quad  triple  ...  Rhode Island  \\\n",
       "0    0.0        0.0        0.0          0     0       1  ...             0   \n",
       "1    0.0        0.0        0.0          1     2       3  ...             0   \n",
       "2    0.0        0.0        0.0          0     0       1  ...             0   \n",
       "\n",
       "   South Dakota  Tennessee  Utah  Vermont  Virginia  Washington  \\\n",
       "0             0          0     0        0         0           0   \n",
       "1             0          0     0        0         0           0   \n",
       "2             0          0     0        0         0           0   \n",
       "\n",
       "   West Virginia  Wisconsin  Wyoming  \n",
       "0              0          0        0  \n",
       "1              0          0        0  \n",
       "2              0          0        0  \n",
       "\n",
       "[3 rows x 61 columns]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = pd.concat([df.drop(['state'],axis=1),pd.get_dummies(df['state'])],axis=1)\n",
    "k.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnDVhE1-ZyGF"
   },
   "source": [
    "## Standardize the magnitude of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW3D-WlDZyGG"
   },
   "source": [
    "**<font color='teal'> Using sklearn preprocessing standardize the scale of the features of the dataframe except the name of the resort which we done't need in the dataframe for modeling, so it can be droppped here as well. Also, we want to hold out our response variable(s) so we can have their true values available for model performance review. Let's set `AdultWeekend` to the y variable as our response for scaling and modeling. Later we will go back and consider the `AdultWeekday`, `dayOpenLastYear`, and `projectedDaysOpen`. For now leave them in the development dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZL-q-KtAYI6"
   },
   "outputs": [],
   "source": [
    "# first we import the preprocessing package from the sklearn library\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df\n",
    "#X = k.drop(['Name','AdultWeekend'], axis=1)\n",
    "X = k.drop(['Name','AdultWeekend'],axis=1)\n",
    "# Declare a response variable, called y, and assign it the AdultWeekend column of the df \n",
    "y = k.AdultWeekend \n",
    "\n",
    "# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \n",
    "X_scaled=scaler.transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAT8h4_mZyGK"
   },
   "source": [
    "## Split into training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rdS8EGeAnrW"
   },
   "source": [
    "**<font color='teal'> Using sklearn model selection import train_test_split, and create a 75/25 split with the y = `AdultWeekend`. We will start by using the adult weekend ticket price as our response variable for modeling.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSkPut0gguds"
   },
   "outputs": [],
   "source": [
    "# Import the train_test_split function from the sklearn.model_selection utility.  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y\n",
    "y = k.AdultWeekend \n",
    "y = k['AdultWeekend'].ravel()\n",
    "\n",
    "# Call the train_test_split() function with the first two parameters set to X_scaled and y \n",
    "# Declare four variables, X_train, X_test, y_train and y_test separated by commas \n",
    "X_train,X_test,y_train,y_test= train_test_split(X_scaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.80077675, -0.85212798, -0.71839355, ..., -0.13168538,\n",
       "        -0.25819889, -0.17099639],\n",
       "       [-0.61050361,  0.55347569, -0.79288315, ..., -0.13168538,\n",
       "        -0.25819889, -0.17099639],\n",
       "       [-0.54151026,  0.19614753, -0.64251809, ..., -0.13168538,\n",
       "        -0.25819889, -0.17099639],\n",
       "       ...,\n",
       "       [-0.80232369, -0.9368029 , -0.7028027 , ..., -0.13168538,\n",
       "        -0.25819889, -0.17099639],\n",
       "       [ 1.51808048,  1.26474502,  1.52564902, ..., -0.13168538,\n",
       "        -0.25819889, -0.17099639],\n",
       "       [-0.81500856, -0.64044068, -0.77763877, ..., -0.13168538,\n",
       "        -0.25819889, -0.17099639]])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UayqbwkWAnra"
   },
   "source": [
    "Here we start the actual modeling work. First let's fit a multiple linear regression model to predict the `AdultWeekend` price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83fkLldXFCNd"
   },
   "source": [
    "# Guided Capstone Step 5. Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbZXsVevfr9M"
   },
   "source": [
    "This is the fifth step in the Data Science Method. In the previous steps you cleaned and prepared the datasets. Now it's time to get into the most exciting part: modeling! In this exercise, you'll build three different models and compare each model's performance. In the end, you'll choose the best model for demonstrating insights to Big Mountain management.\n",
    "\n",
    "\n",
    "\n",
    "### **The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    "3.   Exploratory Data Analysis \n",
    " \n",
    "4.   Pre-processing and Training Data Development\n",
    "\n",
    "5.   **Modeling**\n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes — Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_wfsP_-Anra"
   },
   "source": [
    "## Fit Models with a Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoI8S5SwAnrc"
   },
   "source": [
    "**<font color='teal'> Using sklearn, fit the model on your training dataset.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/a0/f29d1644e74ecac3b86b7135f1f6058050e367568cbc493c981390c8ca34/statsmodels-0.11.1-cp37-cp37m-macosx_10_13_x86_64.whl (8.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.4MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting patsy>=0.5 (from statsmodels)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/0c/5f61f1a3d4385d6bf83b83ea495068857ff8dfb89e74824c6e9eb63286d8/patsy-0.5.1-py2.py3-none-any.whl (231kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 16.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /Users/irenecho/work/venv/py37/lib/python3.7/site-packages (from statsmodels) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /Users/irenecho/work/venv/py37/lib/python3.7/site-packages (from statsmodels) (1.18.4)\n",
      "Requirement already satisfied: pandas>=0.21 in /Users/irenecho/work/venv/py37/lib/python3.7/site-packages (from statsmodels) (1.0.3)\n",
      "Requirement already satisfied: six in /Users/irenecho/work/venv/py37/lib/python3.7/site-packages (from patsy>=0.5->statsmodels) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/irenecho/work/venv/py37/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/irenecho/work/venv/py37/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2020.1)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-0.5.1 statsmodels-0.11.1\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_GFr8sRAnrd"
   },
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW6K7uOPAnre"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 21 Jun 2020</td> <th>  Prob (F-statistic):</th> <td>3.14e-25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:41:04</td>     <th>  Log-Likelihood:    </th> <td> -390.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   132</td>      <th>  AIC:               </th> <td>   888.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    78</td>      <th>  BIC:               </th> <td>   1044.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    53</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   29.5261</td> <td>   58.125</td> <td>    0.508</td> <td> 0.613</td> <td>  -86.192</td> <td>  145.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -2.1138</td> <td>   10.800</td> <td>   -0.196</td> <td> 0.845</td> <td>  -23.615</td> <td>   19.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -26.7983</td> <td>   51.337</td> <td>   -0.522</td> <td> 0.603</td> <td> -129.003</td> <td>   75.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.461e-13</td> <td> 2.78e-14</td> <td>   19.631</td> <td> 0.000</td> <td> 4.91e-13</td> <td> 6.01e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>-6.009e-13</td> <td> 1.25e-14</td> <td>  -47.880</td> <td> 0.000</td> <td>-6.26e-13</td> <td>-5.76e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> 7.557e-14</td> <td> 3.69e-15</td> <td>   20.482</td> <td> 0.000</td> <td> 6.82e-14</td> <td> 8.29e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.1572</td> <td>    0.835</td> <td>    0.188</td> <td> 0.851</td> <td>   -1.506</td> <td>    1.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    1.6591</td> <td>    0.729</td> <td>    2.274</td> <td> 0.026</td> <td>    0.207</td> <td>    3.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    1.3303</td> <td>    0.769</td> <td>    1.730</td> <td> 0.088</td> <td>   -0.201</td> <td>    2.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.9361</td> <td>    0.725</td> <td>    1.291</td> <td> 0.201</td> <td>   -0.508</td> <td>    2.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.9599</td> <td>    0.662</td> <td>   -1.449</td> <td> 0.151</td> <td>   -2.278</td> <td>    0.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    1.0633</td> <td>    0.486</td> <td>    2.186</td> <td> 0.032</td> <td>    0.095</td> <td>    2.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.5766</td> <td>    1.624</td> <td>    0.355</td> <td> 0.723</td> <td>   -2.656</td> <td>    3.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    1.8262</td> <td>    0.858</td> <td>    2.127</td> <td> 0.037</td> <td>    0.117</td> <td>    3.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    1.1278</td> <td>    0.951</td> <td>    1.186</td> <td> 0.239</td> <td>   -0.765</td> <td>    3.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -2.0461</td> <td>    1.159</td> <td>   -1.766</td> <td> 0.081</td> <td>   -4.353</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.3316</td> <td>    0.970</td> <td>   -0.342</td> <td> 0.733</td> <td>   -2.263</td> <td>    1.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    1.2400</td> <td>    1.013</td> <td>    1.224</td> <td> 0.225</td> <td>   -0.777</td> <td>    3.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.5242</td> <td>    0.798</td> <td>   -0.657</td> <td> 0.513</td> <td>   -2.112</td> <td>    1.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    2.0246</td> <td>    1.251</td> <td>    1.618</td> <td> 0.110</td> <td>   -0.466</td> <td>    4.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    9.3715</td> <td>    0.966</td> <td>    9.699</td> <td> 0.000</td> <td>    7.448</td> <td>   11.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -2.2433</td> <td>    1.005</td> <td>   -2.232</td> <td> 0.028</td> <td>   -4.244</td> <td>   -0.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.6793</td> <td>    0.781</td> <td>    0.870</td> <td> 0.387</td> <td>   -0.875</td> <td>    2.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    1.9562</td> <td>    2.710</td> <td>    0.722</td> <td> 0.473</td> <td>   -3.439</td> <td>    7.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    4.4173</td> <td>    0.523</td> <td>    8.449</td> <td> 0.000</td> <td>    3.377</td> <td>    5.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    3.5296</td> <td>    0.787</td> <td>    4.487</td> <td> 0.000</td> <td>    1.964</td> <td>    5.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   13.9158</td> <td>    1.326</td> <td>   10.493</td> <td> 0.000</td> <td>   11.276</td> <td>   16.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    9.4392</td> <td>    1.260</td> <td>    7.492</td> <td> 0.000</td> <td>    6.931</td> <td>   11.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   10.4422</td> <td>    0.765</td> <td>   13.651</td> <td> 0.000</td> <td>    8.919</td> <td>   11.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>    8.7230</td> <td>    0.920</td> <td>    9.481</td> <td> 0.000</td> <td>    6.891</td> <td>   10.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>    7.5737</td> <td>    0.712</td> <td>   10.637</td> <td> 0.000</td> <td>    6.156</td> <td>    8.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    5.6567</td> <td>    0.602</td> <td>    9.403</td> <td> 0.000</td> <td>    4.459</td> <td>    6.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    7.3058</td> <td>    0.714</td> <td>   10.236</td> <td> 0.000</td> <td>    5.885</td> <td>    8.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    9.2645</td> <td>    0.928</td> <td>    9.982</td> <td> 0.000</td> <td>    7.417</td> <td>   11.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    5.1655</td> <td>    0.526</td> <td>    9.816</td> <td> 0.000</td> <td>    4.118</td> <td>    6.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>   10.2907</td> <td>    0.667</td> <td>   15.434</td> <td> 0.000</td> <td>    8.963</td> <td>   11.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   16.2776</td> <td>    0.889</td> <td>   18.312</td> <td> 0.000</td> <td>   14.508</td> <td>   18.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   12.1094</td> <td>    0.697</td> <td>   17.380</td> <td> 0.000</td> <td>   10.722</td> <td>   13.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    6.4439</td> <td>    0.555</td> <td>   11.610</td> <td> 0.000</td> <td>    5.339</td> <td>    7.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    7.2379</td> <td>    1.008</td> <td>    7.183</td> <td> 0.000</td> <td>    5.232</td> <td>    9.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    3.0772</td> <td>    2.780</td> <td>    1.107</td> <td> 0.272</td> <td>   -2.457</td> <td>    8.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   15.8861</td> <td>    0.849</td> <td>   18.719</td> <td> 0.000</td> <td>   14.197</td> <td>   17.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td> -738.8070</td> <td>    7.468</td> <td>  -98.929</td> <td> 0.000</td> <td> -753.675</td> <td> -723.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    9.0087</td> <td>    1.589</td> <td>    5.668</td> <td> 0.000</td> <td>    5.845</td> <td>   12.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   19.5554</td> <td>    0.860</td> <td>   22.744</td> <td> 0.000</td> <td>   17.844</td> <td>   21.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>    9.5469</td> <td>    0.570</td> <td>   16.756</td> <td> 0.000</td> <td>    8.413</td> <td>   10.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>    8.0932</td> <td>    0.740</td> <td>   10.943</td> <td> 0.000</td> <td>    6.621</td> <td>    9.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>    6.5916</td> <td>    0.880</td> <td>    7.489</td> <td> 0.000</td> <td>    4.839</td> <td>    8.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   15.1845</td> <td>    0.811</td> <td>   18.725</td> <td> 0.000</td> <td>   13.570</td> <td>   16.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    5.3920</td> <td>    0.543</td> <td>    9.939</td> <td> 0.000</td> <td>    4.312</td> <td>    6.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>    3.7496</td> <td>    0.602</td> <td>    6.225</td> <td> 0.000</td> <td>    2.550</td> <td>    4.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    6.2695</td> <td>    0.559</td> <td>   11.219</td> <td> 0.000</td> <td>    5.157</td> <td>    7.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    6.6259</td> <td>    0.781</td> <td>    8.488</td> <td> 0.000</td> <td>    5.072</td> <td>    8.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>   12.5935</td> <td>    0.906</td> <td>   13.893</td> <td> 0.000</td> <td>   10.789</td> <td>   14.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    8.2092</td> <td>    0.517</td> <td>   15.879</td> <td> 0.000</td> <td>    7.180</td> <td>    9.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>    7.8264</td> <td>    1.010</td> <td>    7.745</td> <td> 0.000</td> <td>    5.815</td> <td>    9.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>    9.5925</td> <td>    0.650</td> <td>   14.753</td> <td> 0.000</td> <td>    8.298</td> <td>   10.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>   14.6620</td> <td>    0.836</td> <td>   17.528</td> <td> 0.000</td> <td>   12.997</td> <td>   16.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>    8.2441</td> <td>    1.029</td> <td>    8.009</td> <td> 0.000</td> <td>    6.195</td> <td>   10.293</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.548</td> <th>  Durbin-Watson:     </th> <td>   1.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.008</td> <th>  Jarque-Bera (JB):  </th> <td>  15.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.321</td> <th>  Prob(JB):          </th> <td>0.000477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.539</td> <th>  Cond. No.          </th> <td>1.18e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 6.83e-30. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.913\n",
       "Model:                            OLS   Adj. R-squared:                  0.854\n",
       "Method:                 Least Squares   F-statistic:                     15.41\n",
       "Date:                Sun, 21 Jun 2020   Prob (F-statistic):           3.14e-25\n",
       "Time:                        19:41:04   Log-Likelihood:                -390.07\n",
       "No. Observations:                 132   AIC:                             888.1\n",
       "Df Residuals:                      78   BIC:                             1044.\n",
       "Df Model:                          53                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            29.5261     58.125      0.508      0.613     -86.192     145.244\n",
       "x2            -2.1138     10.800     -0.196      0.845     -23.615      19.387\n",
       "x3           -26.7983     51.337     -0.522      0.603    -129.003      75.406\n",
       "const       5.461e-13   2.78e-14     19.631      0.000    4.91e-13    6.01e-13\n",
       "x4         -6.009e-13   1.25e-14    -47.880      0.000   -6.26e-13   -5.76e-13\n",
       "x5          7.557e-14   3.69e-15     20.482      0.000    6.82e-14    8.29e-14\n",
       "x6             0.1572      0.835      0.188      0.851      -1.506       1.820\n",
       "x7             1.6591      0.729      2.274      0.026       0.207       3.111\n",
       "x8             1.3303      0.769      1.730      0.088      -0.201       2.861\n",
       "x9             0.9361      0.725      1.291      0.201      -0.508       2.380\n",
       "x10           -0.9599      0.662     -1.449      0.151      -2.278       0.359\n",
       "x11            1.0633      0.486      2.186      0.032       0.095       2.032\n",
       "x12            0.5766      1.624      0.355      0.723      -2.656       3.809\n",
       "x13            1.8262      0.858      2.127      0.037       0.117       3.535\n",
       "x14            1.1278      0.951      1.186      0.239      -0.765       3.020\n",
       "x15           -2.0461      1.159     -1.766      0.081      -4.353       0.261\n",
       "x16           -0.3316      0.970     -0.342      0.733      -2.263       1.600\n",
       "x17            1.2400      1.013      1.224      0.225      -0.777       3.257\n",
       "x18           -0.5242      0.798     -0.657      0.513      -2.112       1.064\n",
       "x19            2.0246      1.251      1.618      0.110      -0.466       4.515\n",
       "x20            9.3715      0.966      9.699      0.000       7.448      11.295\n",
       "x21           -2.2433      1.005     -2.232      0.028      -4.244      -0.243\n",
       "x22            0.6793      0.781      0.870      0.387      -0.875       2.233\n",
       "x23            1.9562      2.710      0.722      0.473      -3.439       7.352\n",
       "x24            4.4173      0.523      8.449      0.000       3.377       5.458\n",
       "x25            3.5296      0.787      4.487      0.000       1.964       5.096\n",
       "x26           13.9158      1.326     10.493      0.000      11.276      16.556\n",
       "x27            9.4392      1.260      7.492      0.000       6.931      11.948\n",
       "x28           10.4422      0.765     13.651      0.000       8.919      11.965\n",
       "x29            8.7230      0.920      9.481      0.000       6.891      10.555\n",
       "x30            7.5737      0.712     10.637      0.000       6.156       8.991\n",
       "x31            5.6567      0.602      9.403      0.000       4.459       6.854\n",
       "x32            7.3058      0.714     10.236      0.000       5.885       8.727\n",
       "x33            9.2645      0.928      9.982      0.000       7.417      11.112\n",
       "x34            5.1655      0.526      9.816      0.000       4.118       6.213\n",
       "x35           10.2907      0.667     15.434      0.000       8.963      11.618\n",
       "x36           16.2776      0.889     18.312      0.000      14.508      18.047\n",
       "x37           12.1094      0.697     17.380      0.000      10.722      13.496\n",
       "x38            6.4439      0.555     11.610      0.000       5.339       7.549\n",
       "x39            7.2379      1.008      7.183      0.000       5.232       9.244\n",
       "x40            3.0772      2.780      1.107      0.272      -2.457       8.612\n",
       "x41           15.8861      0.849     18.719      0.000      14.197      17.576\n",
       "x42         -738.8070      7.468    -98.929      0.000    -753.675    -723.939\n",
       "x43            9.0087      1.589      5.668      0.000       5.845      12.173\n",
       "x44           19.5554      0.860     22.744      0.000      17.844      21.267\n",
       "x45            9.5469      0.570     16.756      0.000       8.413      10.681\n",
       "x46            8.0932      0.740     10.943      0.000       6.621       9.566\n",
       "x47            6.5916      0.880      7.489      0.000       4.839       8.344\n",
       "x48           15.1845      0.811     18.725      0.000      13.570      16.799\n",
       "x49            5.3920      0.543      9.939      0.000       4.312       6.472\n",
       "x50            3.7496      0.602      6.225      0.000       2.550       4.949\n",
       "x51            6.2695      0.559     11.219      0.000       5.157       7.382\n",
       "x52            6.6259      0.781      8.488      0.000       5.072       8.180\n",
       "x53           12.5935      0.906     13.893      0.000      10.789      14.398\n",
       "x54            8.2092      0.517     15.879      0.000       7.180       9.238\n",
       "x55            7.8264      1.010      7.745      0.000       5.815       9.838\n",
       "x56            9.5925      0.650     14.753      0.000       8.298      10.887\n",
       "x57           14.6620      0.836     17.528      0.000      12.997      16.327\n",
       "x58            8.2441      1.029      8.009      0.000       6.195      10.293\n",
       "==============================================================================\n",
       "Omnibus:                        9.548   Durbin-Watson:                   1.711\n",
       "Prob(Omnibus):                  0.008   Jarque-Bera (JB):               15.297\n",
       "Skew:                          -0.321   Prob(JB):                     0.000477\n",
       "Kurtosis:                       4.539   Cond. No.                     1.18e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 6.83e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all first model set\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.metrics import explained_variance_score,mean_absolute_error\n",
    "# lm = linear_model.LinearRegression()\n",
    "# model = lm.fit(X_train,y_train)\n",
    "import statsmodels.api as sm\n",
    "X_train = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train,X_train)\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fHqz9-WAnrg"
   },
   "source": [
    "**<font color='teal'> Predict on the testing dataset and score the model performance with the y_test set and the y-pred values. The explained variance is a measure of the variation explained by the model. This is also known as the R-squared value. </font>**\n",
    "\n",
    "Hint: you will have to use the `predict()` method here as it's used in this [DSM article](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-modeling-56b4233cad1b) about modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIo01lFEAnrh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test\\\n",
    "y_pred =model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4YS0WE2Anrk"
   },
   "source": [
    "## Review Model Outcomes — Iterate over additional models as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSh9sGIYAnrk"
   },
   "outputs": [],
   "source": [
    "# You might want to use the explained_variance_score() and mean_absolute_error() metrics.\n",
    "# To do so, you will need to import them from sklearn.metrics. \n",
    "# You can plug y_test and y_pred into the functions to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihzeo8tqAnro"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-11706.639930117824, 231.7118369096451)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error\n",
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWJcOuSdAnrr"
   },
   "source": [
    "**<font color='teal'> Print the intercept value from the linear model. </font>**\n",
    "\n",
    "Hint: our linear regression model `lm` has an attribute `intercept_` for the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.460818146023717e-13"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edajrenAAnrv"
   },
   "source": [
    "**<font color='teal'> The intercept is the mean `AdultWeekend` price for all the resorts given the other characteristics. The addition or subtraction of each of the coefficient values in the regression are numeric adjustments applied to the intercept to provide a particular observation's value for the resulting `AdultWeekend` value. Also, because we took the time to scale our x values in the training data, we can compare each of the coeeficients for the features to determine the feature importances. Print the coefficient values from the linear model and sort in descending order to identify the top ten most important features.</font>** \n",
    "\n",
    "\n",
    "Hint: make sure to review the absolute value of the coefficients, because the adjustment may be positive or negative, but what we are looking for is the magnitude of impact on our response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEKc_lmZAnrw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>738.807029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>29.526120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_elev</th>\n",
       "      <td>26.798305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>19.555365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>16.277611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>15.886130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>15.184512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>14.662033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>13.915831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>12.593484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Coefficient\n",
       "New Jersey      738.807029\n",
       "summit_elev      29.526120\n",
       "base_elev        26.798305\n",
       "New York         19.555365\n",
       "Michigan         16.277611\n",
       "New Hampshire    15.886130\n",
       "Pennsylvania     15.184512\n",
       "Wisconsin        14.662033\n",
       "California       13.915831\n",
       "Vermont          12.593484"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You might want to make a pandas DataFrame displaying the coefficients for each state like so: \n",
    "df1 = pd.DataFrame(abs(model.params), X.columns,columns=['Coefficient'])\n",
    "df1.sort_values(by = 'Coefficient',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpdALMoAAnry"
   },
   "source": [
    "**<font color='teal'>You should see that the top ten important features are different states. However, the state is not something the managers at the Big Mountain Resort can do anything about. Given that we care more about actionable traits associated with ticket pricing, rebuild the model without the state features and compare the results. </font>**\n",
    "\n",
    "Hint: Try to construct another model using exactly the steps we followed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mHYA1BzAnrz"
   },
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pz1YXAdiAnr0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['New Jersey', 'summit_elev', 'base_elev', 'New York', 'Michigan',\n",
       "       'New Hampshire', 'Pennsylvania', 'Wisconsin', 'California', 'Vermont'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = df1.sort_values(by = 'Coefficient',ascending=False).head(10).index\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.concat([df.drop(['state'],axis=1),pd.get_dummies(df['state'])],axis=1)\n",
    "X = k.drop(['New Jersey', 'New York', 'Michigan', 'New Hampshire', 'Pennsylvania',\n",
    "       'Wisconsin', 'California', 'Vermont','Name','AdultWeekend'],axis=1)\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) \n",
    "y = k.AdultWeekend \n",
    "y = k['AdultWeekend'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size=0.25,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM1EGf16Anr2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.61</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 21 Jun 2020</td> <th>  Prob (F-statistic):</th> <td>4.92e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:07:00</td>     <th>  Log-Likelihood:    </th> <td> -411.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   132</td>      <th>  AIC:               </th> <td>   914.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    86</td>      <th>  BIC:               </th> <td>   1047.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    45</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  -30.7265</td> <td>   52.926</td> <td>   -0.581</td> <td> 0.563</td> <td> -135.940</td> <td>   74.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    9.7037</td> <td>    9.858</td> <td>    0.984</td> <td> 0.328</td> <td>   -9.893</td> <td>   29.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   21.5869</td> <td>   47.274</td> <td>    0.457</td> <td> 0.649</td> <td>  -72.391</td> <td>  115.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-2.555e-13</td> <td> 1.83e-14</td> <td>  -13.981</td> <td> 0.000</td> <td>-2.92e-13</td> <td>-2.19e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> 4.964e-13</td> <td> 2.72e-14</td> <td>   18.224</td> <td> 0.000</td> <td> 4.42e-13</td> <td> 5.51e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-4.458e-13</td> <td> 9.87e-15</td> <td>  -45.192</td> <td> 0.000</td> <td>-4.65e-13</td> <td>-4.26e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    1.3058</td> <td>    0.921</td> <td>    1.417</td> <td> 0.160</td> <td>   -0.526</td> <td>    3.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    1.6902</td> <td>    0.797</td> <td>    2.120</td> <td> 0.037</td> <td>    0.106</td> <td>    3.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    1.4801</td> <td>    0.803</td> <td>    1.844</td> <td> 0.069</td> <td>   -0.116</td> <td>    3.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.5496</td> <td>    0.663</td> <td>    0.829</td> <td> 0.409</td> <td>   -0.768</td> <td>    1.867</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -1.4534</td> <td>    0.690</td> <td>   -2.107</td> <td> 0.038</td> <td>   -2.825</td> <td>   -0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.9121</td> <td>    0.455</td> <td>    2.004</td> <td> 0.048</td> <td>    0.007</td> <td>    1.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    2.1433</td> <td>    1.476</td> <td>    1.452</td> <td> 0.150</td> <td>   -0.790</td> <td>    5.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.4348</td> <td>    0.815</td> <td>    0.533</td> <td> 0.595</td> <td>   -1.186</td> <td>    2.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    1.2436</td> <td>    0.989</td> <td>    1.257</td> <td> 0.212</td> <td>   -0.723</td> <td>    3.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -3.1139</td> <td>    1.365</td> <td>   -2.281</td> <td> 0.025</td> <td>   -5.827</td> <td>   -0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.3758</td> <td>    1.024</td> <td>   -0.367</td> <td> 0.715</td> <td>   -2.411</td> <td>    1.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.2359</td> <td>    1.068</td> <td>    0.221</td> <td> 0.826</td> <td>   -1.888</td> <td>    2.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.8231</td> <td>    0.789</td> <td>    1.044</td> <td> 0.300</td> <td>   -0.745</td> <td>    2.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.1683</td> <td>    1.295</td> <td>    0.130</td> <td> 0.897</td> <td>   -2.406</td> <td>    2.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    8.3836</td> <td>    1.071</td> <td>    7.825</td> <td> 0.000</td> <td>    6.254</td> <td>   10.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -1.5498</td> <td>    1.195</td> <td>   -1.297</td> <td> 0.198</td> <td>   -3.924</td> <td>    0.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.0996</td> <td>    0.761</td> <td>    0.131</td> <td> 0.896</td> <td>   -1.414</td> <td>    1.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    8.1714</td> <td>    4.689</td> <td>    1.743</td> <td> 0.085</td> <td>   -1.150</td> <td>   17.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>   -0.0729</td> <td>    0.587</td> <td>   -0.124</td> <td> 0.901</td> <td>   -1.240</td> <td>    1.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td> -374.6468</td> <td>    4.649</td> <td>  -80.581</td> <td> 0.000</td> <td> -383.889</td> <td> -365.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -3.3680</td> <td>    1.922</td> <td>   -1.753</td> <td> 0.083</td> <td>   -7.188</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.6434</td> <td>    0.757</td> <td>    0.850</td> <td> 0.398</td> <td>   -0.862</td> <td>    2.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -2.3275</td> <td>    0.981</td> <td>   -2.372</td> <td> 0.020</td> <td>   -4.278</td> <td>   -0.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.6657</td> <td>    0.588</td> <td>   -1.133</td> <td> 0.260</td> <td>   -1.834</td> <td>    0.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.9570</td> <td>    0.585</td> <td>   -1.635</td> <td> 0.106</td> <td>   -2.120</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.5266</td> <td>    0.581</td> <td>   -0.907</td> <td> 0.367</td> <td>   -1.681</td> <td>    0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -1.7226</td> <td>    1.258</td> <td>   -1.370</td> <td> 0.174</td> <td>   -4.223</td> <td>    0.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.8720</td> <td>    0.596</td> <td>    1.463</td> <td> 0.147</td> <td>   -0.313</td> <td>    2.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.4769</td> <td>    0.774</td> <td>    0.616</td> <td> 0.539</td> <td>   -1.061</td> <td>    2.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>   -0.7411</td> <td>    0.613</td> <td>   -1.209</td> <td> 0.230</td> <td>   -1.960</td> <td>    0.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0829</td> <td>    0.576</td> <td>   -0.144</td> <td> 0.886</td> <td>   -1.228</td> <td>    1.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -2.4975</td> <td>    0.893</td> <td>   -2.796</td> <td> 0.006</td> <td>   -4.273</td> <td>   -0.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td> -374.6468</td> <td>    4.649</td> <td>  -80.581</td> <td> 0.000</td> <td> -383.889</td> <td> -365.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -3.2976</td> <td>    1.588</td> <td>   -2.076</td> <td> 0.041</td> <td>   -6.455</td> <td>   -0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    1.1242</td> <td>    0.876</td> <td>    1.284</td> <td> 0.203</td> <td>   -0.616</td> <td>    2.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -0.7873</td> <td>    0.602</td> <td>   -1.308</td> <td> 0.194</td> <td>   -1.984</td> <td>    0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.1259</td> <td>    0.829</td> <td>   -0.152</td> <td> 0.880</td> <td>   -1.774</td> <td>    1.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.6437</td> <td>    0.539</td> <td>    1.195</td> <td> 0.235</td> <td>   -0.427</td> <td>    1.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -1.2169</td> <td>    0.654</td> <td>   -1.862</td> <td> 0.066</td> <td>   -2.516</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>    1.8803</td> <td>    0.628</td> <td>    2.995</td> <td> 0.004</td> <td>    0.632</td> <td>    3.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -1.2015</td> <td>    0.712</td> <td>   -1.688</td> <td> 0.095</td> <td>   -2.616</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>    1.6414</td> <td>    0.550</td> <td>    2.985</td> <td> 0.004</td> <td>    0.548</td> <td>    2.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -1.3331</td> <td>    0.951</td> <td>   -1.401</td> <td> 0.165</td> <td>   -3.224</td> <td>    0.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    3.1054</td> <td>    1.001</td> <td>    3.103</td> <td> 0.003</td> <td>    1.116</td> <td>    5.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -2.7190</td> <td>    1.048</td> <td>   -2.595</td> <td> 0.011</td> <td>   -4.802</td> <td>   -0.636</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.511</td> <th>  Durbin-Watson:     </th> <td>   2.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.023</td> <th>  Jarque-Bera (JB):  </th> <td>  11.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.224</td> <th>  Prob(JB):          </th> <td> 0.00296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.385</td> <th>  Cond. No.          </th> <td>1.21e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 5.97e-30. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.868\n",
       "Model:                            OLS   Adj. R-squared:                  0.800\n",
       "Method:                 Least Squares   F-statistic:                     12.61\n",
       "Date:                Sun, 21 Jun 2020   Prob (F-statistic):           4.92e-23\n",
       "Time:                        20:07:00   Log-Likelihood:                -411.39\n",
       "No. Observations:                 132   AIC:                             914.8\n",
       "Df Residuals:                      86   BIC:                             1047.\n",
       "Df Model:                          45                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1           -30.7265     52.926     -0.581      0.563    -135.940      74.487\n",
       "x2             9.7037      9.858      0.984      0.328      -9.893      29.300\n",
       "x3            21.5869     47.274      0.457      0.649     -72.391     115.565\n",
       "const      -2.555e-13   1.83e-14    -13.981      0.000   -2.92e-13   -2.19e-13\n",
       "x4          4.964e-13   2.72e-14     18.224      0.000    4.42e-13    5.51e-13\n",
       "x5         -4.458e-13   9.87e-15    -45.192      0.000   -4.65e-13   -4.26e-13\n",
       "x6             1.3058      0.921      1.417      0.160      -0.526       3.137\n",
       "x7             1.6902      0.797      2.120      0.037       0.106       3.275\n",
       "x8             1.4801      0.803      1.844      0.069      -0.116       3.076\n",
       "x9             0.5496      0.663      0.829      0.409      -0.768       1.867\n",
       "x10           -1.4534      0.690     -2.107      0.038      -2.825      -0.082\n",
       "x11            0.9121      0.455      2.004      0.048       0.007       1.817\n",
       "x12            2.1433      1.476      1.452      0.150      -0.790       5.077\n",
       "x13            0.4348      0.815      0.533      0.595      -1.186       2.056\n",
       "x14            1.2436      0.989      1.257      0.212      -0.723       3.211\n",
       "x15           -3.1139      1.365     -2.281      0.025      -5.827      -0.400\n",
       "x16           -0.3758      1.024     -0.367      0.715      -2.411       1.660\n",
       "x17            0.2359      1.068      0.221      0.826      -1.888       2.360\n",
       "x18            0.8231      0.789      1.044      0.300      -0.745       2.391\n",
       "x19            0.1683      1.295      0.130      0.897      -2.406       2.743\n",
       "x20            8.3836      1.071      7.825      0.000       6.254      10.513\n",
       "x21           -1.5498      1.195     -1.297      0.198      -3.924       0.825\n",
       "x22            0.0996      0.761      0.131      0.896      -1.414       1.613\n",
       "x23            8.1714      4.689      1.743      0.085      -1.150      17.492\n",
       "x24           -0.0729      0.587     -0.124      0.901      -1.240       1.094\n",
       "x25         -374.6468      4.649    -80.581      0.000    -383.889    -365.404\n",
       "x26           -3.3680      1.922     -1.753      0.083      -7.188       0.452\n",
       "x27            0.6434      0.757      0.850      0.398      -0.862       2.148\n",
       "x28           -2.3275      0.981     -2.372      0.020      -4.278      -0.377\n",
       "x29           -0.6657      0.588     -1.133      0.260      -1.834       0.502\n",
       "x30           -0.9570      0.585     -1.635      0.106      -2.120       0.206\n",
       "x31           -0.5266      0.581     -0.907      0.367      -1.681       0.628\n",
       "x32           -1.7226      1.258     -1.370      0.174      -4.223       0.777\n",
       "x33            0.8720      0.596      1.463      0.147      -0.313       2.057\n",
       "x34            0.4769      0.774      0.616      0.539      -1.061       2.015\n",
       "x35           -0.7411      0.613     -1.209      0.230      -1.960       0.478\n",
       "x36           -0.0829      0.576     -0.144      0.886      -1.228       1.062\n",
       "x37           -2.4975      0.893     -2.796      0.006      -4.273      -0.722\n",
       "x38         -374.6468      4.649    -80.581      0.000    -383.889    -365.404\n",
       "x39           -3.2976      1.588     -2.076      0.041      -6.455      -0.140\n",
       "x40            1.1242      0.876      1.284      0.203      -0.616       2.865\n",
       "x41           -0.7873      0.602     -1.308      0.194      -1.984       0.409\n",
       "x42           -0.1259      0.829     -0.152      0.880      -1.774       1.523\n",
       "x43            0.6437      0.539      1.195      0.235      -0.427       1.714\n",
       "x44           -1.2169      0.654     -1.862      0.066      -2.516       0.082\n",
       "x45            1.8803      0.628      2.995      0.004       0.632       3.128\n",
       "x46           -1.2015      0.712     -1.688      0.095      -2.616       0.213\n",
       "x47            1.6414      0.550      2.985      0.004       0.548       2.734\n",
       "x48           -1.3331      0.951     -1.401      0.165      -3.224       0.558\n",
       "x49            3.1054      1.001      3.103      0.003       1.116       5.095\n",
       "x50           -2.7190      1.048     -2.595      0.011      -4.802      -0.636\n",
       "==============================================================================\n",
       "Omnibus:                        7.511   Durbin-Watson:                   2.268\n",
       "Prob(Omnibus):                  0.023   Jarque-Bera (JB):               11.648\n",
       "Skew:                           0.224   Prob(JB):                      0.00296\n",
       "Kurtosis:                       4.385   Cond. No.                     1.21e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.97e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X_train = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train,X_train)\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5013.416582311873, 234.56700108921552)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =model.predict(X_test)\n",
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>374.646823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>374.646823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>30.726548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_elev</th>\n",
       "      <td>21.586860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>9.703731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>8.383618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusters</th>\n",
       "      <td>8.171367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>3.368048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>3.297554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>3.113892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Coefficient\n",
       "Arizona             374.646823\n",
       "Nevada              374.646823\n",
       "summit_elev          30.726548\n",
       "base_elev            21.586860\n",
       "vertical_drop         9.703731\n",
       "AdultWeekday          8.383618\n",
       "clusters              8.171367\n",
       "Colorado              3.368048\n",
       "New Mexico            3.297554\n",
       "SkiableTerrain_ac     3.113892"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)\n",
    "df1 = pd.DataFrame(abs(model.params), X.columns,columns=['Coefficient'])\n",
    "df1.sort_values(by = 'Coefficient',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWjQLr3LAnr6"
   },
   "source": [
    "**<font color='teal'> When reviewing our new model coefficients, we see `summit_elev` is now in the number two spot. This is also difficult to change from a management prespective and highly correlated with `base_elev` and `vertical_drop`.  This time, rebuild the model without the state features and without the `summit_elev` and without `base_elev`and compare the results. </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXqvcn93Anr7"
   },
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eugnDNNAnr8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trams</th>\n",
       "      <td>2.692339e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastEight</th>\n",
       "      <td>2.186399e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastSixes</th>\n",
       "      <td>7.788280e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>7.255297e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>3.983055e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>3.811707e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>3.253419e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>1.852512e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>1.548332e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>9.413681e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Coefficient\n",
       "trams         2.692339e+13\n",
       "fastEight     2.186399e+13\n",
       "fastSixes     7.788280e+12\n",
       "total_chairs  7.255297e+12\n",
       "double        3.983055e+12\n",
       "surface       3.811707e+12\n",
       "triple        3.253419e+12\n",
       "quad          1.852512e+12\n",
       "fastQuads     1.548332e+12\n",
       "AdultWeekday  9.413681e+00"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = X.drop(['summit_elev','base_elev'],axis=1)\n",
    "scaler = preprocessing.StandardScaler().fit(X1)\n",
    "X_scaled=scaler.transform(X1) \n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.25,random_state=1)\n",
    "model = lm.fit(X_train,y_train)\n",
    "df2 = pd.DataFrame(abs(model.coef_),X1.columns,columns=['Coefficient'])\n",
    "df2.sort_values(by = 'Coefficient',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5758666278241996, 7.0302538548681754)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =model.predict(X_test)\n",
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJvQMns6AnsI"
   },
   "source": [
    "## Identify the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD7-3iLcAnsK"
   },
   "source": [
    "**<font color='teal'> Review the model performances in the table below and choose the best model for proving insights to Big Mountain management about what features are driving ski resort lift ticket prices. Type your choice in the final markdown cell — you will discuss this selection more in the next step of the guided casptone. </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La5S9fRPAnsK"
   },
   "source": [
    "| Model | Explained Variance| Mean Absolute Error|Features Dropped|\n",
    "| --- | --- | --- | --- |\n",
    "| Model 1. | -5040.51| 234.799 |-|\n",
    "| Model 2. | 0.46309654287415447| 8.755474556976871|'state'|\n",
    "| Model 3. | 0.5627744815474893| 6.944734699845861|'state','summit_elev','base_elev'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2c-zn7TAnsL"
   },
   "source": [
    "Model Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>371.237817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>371.237817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>19.589327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>15.862321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>15.760546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>15.508544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>15.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>14.948883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>14.130724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>13.567745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Coefficient\n",
       "Arizona         371.237817\n",
       "Nevada          371.237817\n",
       "New York         19.589327\n",
       "Michigan         15.862321\n",
       "New Hampshire    15.760546\n",
       "Wisconsin        15.508544\n",
       "Pennsylvania     15.231000\n",
       "California       14.948883\n",
       "Vermont          14.130724\n",
       "summit_elev      13.567745"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import explained_variance_score,mean_absolute_error\n",
    "lm = linear_model.LinearRegression()\n",
    "k = pd.concat([df.drop(['state'],axis=1),pd.get_dummies(df['state'])],axis=1)\n",
    "X = k.drop(['Name','AdultWeekend'],axis=1)\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) \n",
    "y = k.AdultWeekend \n",
    "y = k['AdultWeekend'].ravel()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size=0.25,random_state=12)\n",
    "X_train = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train,X_train)\n",
    "model = model.fit()\n",
    "df2 = pd.DataFrame(abs(model.params),X.columns,columns=['Coefficient'])\n",
    "df2.sort_values(by = 'Coefficient',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5040.51446855076, 234.79949149815567)"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =model.predict(X_test)\n",
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgC0eMBrAnsM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>25.544264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_elev</th>\n",
       "      <td>16.586374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusters</th>\n",
       "      <td>8.789735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>8.286916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>8.262943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>3.334732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>3.233489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>3.170687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>2.798442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>2.610691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Coefficient\n",
       "summit_elev          25.544264\n",
       "base_elev            16.586374\n",
       "clusters              8.789735\n",
       "AdultWeekday          8.286916\n",
       "vertical_drop         8.262943\n",
       "New Mexico            3.334732\n",
       "West Virginia         3.233489\n",
       "Colorado              3.170687\n",
       "SkiableTerrain_ac     2.798442\n",
       "Wyoming               2.610691"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = pd.concat([df.drop(['state'],axis=1),pd.get_dummies(df['state'])],axis=1)\n",
    "X = k.drop(['New Jersey', 'New York', 'Michigan', 'New Hampshire', 'Pennsylvania',\n",
    "       'Wisconsin', 'California','Arizona','Nevada','Name','AdultWeekend'],axis=1)\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) \n",
    "y = k.AdultWeekend \n",
    "y = k['AdultWeekend'].ravel()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size=0.25,random_state=12)\n",
    "# X_train = sm.add_constant(X_train)\n",
    "# model = sm.OLS(y_train,X_train)\n",
    "model = lm.fit(X_train,y_train)\n",
    "df2 = pd.DataFrame(abs(model.coef_),X.columns,columns=['Coefficient'])\n",
    "df2.sort_values(by = 'Coefficient',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46309654287415447, 8.755474556976871)"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =model.predict(X_test)\n",
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fastSixes</th>\n",
       "      <td>4.293098e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>3.738967e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>2.052640e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>1.964337e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>1.676627e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastEight</th>\n",
       "      <td>1.334529e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trams</th>\n",
       "      <td>1.067782e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>9.546793e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>7.979222e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>9.709484e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Coefficient\n",
       "fastSixes     4.293098e+13\n",
       "total_chairs  3.738967e+13\n",
       "double        2.052640e+13\n",
       "surface       1.964337e+13\n",
       "triple        1.676627e+13\n",
       "fastEight     1.334529e+13\n",
       "trams         1.067782e+13\n",
       "quad          9.546793e+12\n",
       "fastQuads     7.979222e+12\n",
       "AdultWeekday  9.709484e+00"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.drop(['summit_elev','base_elev'],axis=1)\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) \n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.25,random_state=1)\n",
    "model = lm.fit(X_train,y_train)\n",
    "df2 = pd.DataFrame(abs(model.coef_),X.columns,columns=['Coefficient'])\n",
    "df2.sort_values(by = 'Coefficient',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5627744815474893, 6.944734699845861)"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred =model.predict(X_test)\n",
    "explained_variance_score(y_test,y_pred), mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RtEspslPZyGY",
    "s0DokMkAZyGc",
    "2iuitnKcZyHS",
    "iAWQxougZyHW",
    "ThMTimlBZyHZ",
    "QwZ-LkjXZyHt",
    "srtXEA3N4-Y9",
    "ChVreJupZyIA",
    "zDgSSsq1ZyID",
    "I3GYKWfi5Llg",
    "pmMvrhbI-viE",
    "ZXDPkW3UZyIX",
    "Dnc_vHQLZyId",
    "daJxuJ-dZyIg",
    "mAQ-oHiPZyIn",
    "hnGOsp3mZyIp"
   ],
   "name": "GuidedCapstoneStep5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
